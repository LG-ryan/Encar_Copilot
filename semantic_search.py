"""
RAG ê¸°ë°˜ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ v2.0
- ìì—°ì–´ MD íŒŒì¼ ì§€ì›
- H3/H4 ê¸°ë°˜ Chunk ë¶„í• 
- Multi-chunk ë‹µë³€ ì¡°í•©
- LLM ì—†ì´ ë™ì‘ (Lightweight RAG)
"""
import os
import json
import pickle
import re
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


class SemanticSearchEngineRAG:
    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        """
        RAG ê¸°ë°˜ í•œêµ­ì–´ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        """
        print("ğŸ”„ ì‹œë§¨í‹± ê²€ìƒ‰ ëª¨ë¸ ë¡œë”© ì¤‘ (RAG ë²„ì „)...")
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.metadata = []
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def load_markdown_file(self, file_path: str) -> List[Dict]:
        """
        ìì—°ì–´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì˜ë¯¸ ë‹¨ìœ„(Semantic Chunk)ë¡œ ë¶„í• 
        
        ë¶„í•  ì „ëµ:
        - H2: ì¹´í…Œê³ ë¦¬ (ë©”íƒ€ë°ì´í„°)
        - H3: Primary Chunk (ë…ë¦½ì ì¸ ì£¼ì œ)
        - H4: Secondary Chunk (H3ì˜ ì„¸ë¶€ ì£¼ì œ, H3ë³´ë‹¤ ìš°ì„ )
        - H5: Tertiary Chunk (H4ì˜ ì„¸ë¶€ ì£¼ì œ, H4ë³´ë‹¤ ìš°ì„ )
        
        ìš°ì„ ìˆœìœ„: H5 > H4 > H3
        """
        print(f"ğŸ“„ MD íŒŒì¼ ë¡œë”© (RAG): {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        chunks = []
        current_h2 = ""
        current_h3 = ""
        current_h3_content = ""
        current_h4 = ""
        current_h4_content = ""
        
        lines = content.split('\n')
        i = 0
        
        while i < len(lines):
            line = lines[i]
            
            # í˜ì´ì§€/ì´ë¯¸ì§€ ë¼ì¸ ê±´ë„ˆë›°ê¸°
            if self._should_skip_line(line):
                i += 1
                continue
            
            # H2: ì¹´í…Œê³ ë¦¬ (ì²­í¬ ìƒì„± ì•ˆí•¨, ë©”íƒ€ë°ì´í„°ë§Œ)
            if line.startswith('## ') and not line.startswith('###'):
                current_h2 = line.replace('##', '').strip()
                i += 1
                continue
            
            # H5: ê°€ì¥ ì„¸ë¶€ì ì¸ ì£¼ì œ (ìµœìš°ì„  ì²­í¬)
            if line.startswith('##### '):
                # ì´ì „ H4 ì²­í¬ ì €ì¥
                if current_h4_content.strip():
                    chunks.append(self._create_chunk(
                        h2=current_h2,
                        h3=current_h3,
                        h4=current_h4,
                        content=current_h4_content,
                        level='h4',
                        source=file_path
                    ))
                    current_h4_content = ""
                
                # H5 ì²­í¬ ìˆ˜ì§‘
                h5_title = line.replace('#####', '').strip()
                h5_content = ""
                i += 1
                
                # H5 ë‚´ìš© ìˆ˜ì§‘ (ë‹¤ìŒ í—¤ë” ì „ê¹Œì§€)
                while i < len(lines) and not self._is_header(lines[i]):
                    if not self._should_skip_line(lines[i]):
                        h5_content += lines[i] + '\n'
                    i += 1
                
                # H5 ì²­í¬ ìƒì„±
                chunks.append(self._create_chunk(
                    h2=current_h2,
                    h3=current_h3,
                    h4=current_h4,
                    h5=h5_title,
                    content=h5_content,
                    level='h5',
                    source=file_path
                ))
                continue
            
            # H4: ì„¸ë¶€ ì£¼ì œ (H3ë³´ë‹¤ ìš°ì„ )
            if line.startswith('#### '):
                # ì´ì „ H4 ì²­í¬ ì €ì¥
                if current_h4_content.strip():
                    chunks.append(self._create_chunk(
                        h2=current_h2,
                        h3=current_h3,
                        h4=current_h4,
                        content=current_h4_content,
                        level='h4',
                        source=file_path
                    ))
                
                # ìƒˆ H4 ì‹œì‘
                current_h4 = line.replace('####', '').strip()
                current_h4_content = ""
                i += 1
                continue
            
            # H3: ì£¼ìš” ì£¼ì œ
            if line.startswith('### '):
                # ì´ì „ H4 ì²­í¬ ì €ì¥
                if current_h4_content.strip():
                    chunks.append(self._create_chunk(
                        h2=current_h2,
                        h3=current_h3,
                        h4=current_h4,
                        content=current_h4_content,
                        level='h4',
                        source=file_path
                    ))
                    current_h4 = ""
                    current_h4_content = ""
                
                # ì´ì „ H3 ì²­í¬ ì €ì¥ (H4ê°€ ì—†ì—ˆë˜ ê²½ìš°)
                elif current_h3_content.strip():
                    chunks.append(self._create_chunk(
                        h2=current_h2,
                        h3=current_h3,
                        content=current_h3_content,
                        level='h3',
                        source=file_path
                    ))
                
                # ìƒˆ H3 ì‹œì‘
                current_h3 = line.replace('###', '').strip()
                current_h3_content = ""
                i += 1
                continue
            
            # ì¼ë°˜ ë‚´ìš© ìˆ˜ì§‘
            if current_h4:
                # H4ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ H4 ë‚´ìš©ìœ¼ë¡œ
                current_h4_content += line + '\n'
            elif current_h3:
                # H3ë§Œ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ H3 ë‚´ìš©ìœ¼ë¡œ
                current_h3_content += line + '\n'
            
            i += 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_h4_content.strip():
            chunks.append(self._create_chunk(
                h2=current_h2,
                h3=current_h3,
                h4=current_h4,
                content=current_h4_content,
                level='h4',
                source=file_path
            ))
        elif current_h3_content.strip():
            chunks.append(self._create_chunk(
                h2=current_h2,
                h3=current_h3,
                content=current_h3_content,
                level='h3',
                source=file_path
            ))
        
        # ë¹ˆ ì²­í¬ í•„í„°ë§
        chunks = [c for c in chunks if c['content'].strip()]
        
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ (RAG)")
        return chunks
    
    def _should_skip_line(self, line: str) -> bool:
        """ê±´ë„ˆë›¸ ë¼ì¸ íŒë‹¨"""
        stripped = line.strip()
        return (
            (stripped.startswith('[Page') and stripped.endswith(']')) or
            stripped.startswith('![ì´ë¯¸ì§€](data:image') or
            stripped.startswith('![ì´ë¯¸ì§€](page')
        )
    
    def _is_header(self, line: str) -> bool:
        """í—¤ë” ë¼ì¸ íŒë‹¨"""
        return line.startswith('#')
    
    def _create_chunk(self, h2: str, h3: str, content: str, level: str, 
                     source: str, h4: str = "", h5: str = "") -> Dict:
        """
        ì²­í¬ ìƒì„±
        
        level: 'h3', 'h4', 'h5'
        """
        # ì œëª© ê²°ì • (ê°€ì¥ ê¹Šì€ ë ˆë²¨ ìš°ì„ )
        if level == 'h5' and h5:
            title = h5
            parent = h4 if h4 else h3
        elif level == 'h4' and h4:
            title = h4
            parent = h3
        else:
            title = h3
            parent = h2
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(f"{h2} {h3} {h4} {h5} {content[:200]}")
        
        # **ì§ˆë¬¸:** / **ë‹µë³€:** í˜•ì‹ í™•ì¸ ë° ì¶”ì¶œ
        question = ""
        answer = ""
        if '**ì§ˆë¬¸:**' in content and '**ë‹µë³€:**' in content:
            qa_match = re.search(r'\*\*ì§ˆë¬¸:\*\*\s*(.+?)\s*\*\*ë‹µë³€:\*\*\s*(.+)', 
                                content, re.DOTALL)
            if qa_match:
                question = qa_match.group(1).strip()
                answer = qa_match.group(2).strip()
        
        return {
            'h2': h2,  # ì¹´í…Œê³ ë¦¬
            'h3': h3,  # ì£¼ìš” ì£¼ì œ
            'h4': h4,  # ì„¸ë¶€ ì£¼ì œ
            'h5': h5,  # ë” ì„¸ë¶€ ì£¼ì œ
            'title': title,  # ì²­í¬ ì œëª© (ê°€ì¥ ê¹Šì€ ë ˆë²¨)
            'content': content.strip(),
            'level': level,  # 'h3', 'h4', 'h5'
            'parent': parent,  # ë¶€ëª¨ ì£¼ì œ
            'keywords': keywords,
            'source': source,
            'question': question,  # FAQ í˜•ì‹ì¸ ê²½ìš°
            'answer': answer,  # FAQ í˜•ì‹ì¸ ê²½ìš°
            'chunk_type': 'qa' if question else 'natural'  # ì²­í¬ íƒ€ì…
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        keywords = list(set([w for w in words if len(w) >= 2]))
        return keywords[:20]
    
    def load_all_markdown_files(self, directory: str = 'docs') -> List[Dict]:
        """docs í´ë”ì˜ ëª¨ë“  MD íŒŒì¼ ë¡œë”©"""
        from pathlib import Path
        all_chunks = []
        
        md_files = list(Path(directory).glob('*.md'))
        print(f"ğŸ“‚ {len(md_files)}ê°œì˜ MD íŒŒì¼ ë°œê²¬: {[f.name for f in md_files]}")
        
        for md_file in sorted(md_files):
            try:
                chunks = self.load_markdown_file(str(md_file))
                all_chunks.extend(chunks)
            except Exception as e:
                print(f"âš ï¸  {md_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ‰ ì´ {len(all_chunks)}ê°œ ì²­í¬ ë¡œë”© ì™„ë£Œ (RAG)!")
        return all_chunks
    
    def build_index(self, documents: List[Dict]):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (RAG)...")
        
        self.documents = documents
        self.metadata = documents
        
        # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„± (ì œëª© + ë‚´ìš© + ê³„ì¸µ ì •ë³´)
        texts = []
        for doc in documents:
            # ê³„ì¸µ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
            hierarchy = f"{doc['h2']} > {doc['h3']}"
            if doc['h4']:
                hierarchy += f" > {doc['h4']}"
            if doc['h5']:
                hierarchy += f" > {doc['h5']}"
            
            # FAQ í˜•ì‹ì¸ ê²½ìš° ì§ˆë¬¸ ìš°ì„ 
            if doc['chunk_type'] == 'qa' and doc['question']:
                text = f"{hierarchy}\nì§ˆë¬¸: {doc['question']}\në‹µë³€: {doc['content']}"
            else:
                text = f"{hierarchy}\n{doc['title']}\n{doc['content']}"
            
            texts.append(text)
        
        # ë²¡í„°ë¡œ ë³€í™˜
        print("â³ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! (ì´ {len(documents)}ê°œ ë¬¸ì„œ)")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰"""
        if self.index is None:
            raise ValueError("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.model.encode([query])
        faiss.normalize_L2(query_vector)
        
        # ê²€ìƒ‰
        scores, indices = self.index.search(query_vector.astype('float32'), top_k)
        
        # ê²°ê³¼ ë°˜í™˜
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx != -1:
                results.append((self.metadata[idx], float(score)))
        
        return results
    
    def save_index(self, path: str = 'data/semantic_index_rag'):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        faiss.write_index(self.index, f'{path}/faiss.index')
        
        with open(f'{path}/metadata.pkl', 'wb') as f:
            pickle.dump(self.metadata, f)
        
        print(f"ğŸ’¾ RAG ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_index(self, path: str = 'data/semantic_index_rag'):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        self.index = faiss.read_index(f'{path}/faiss.index')
        
        with open(f'{path}/metadata.pkl', 'rb') as f:
            self.metadata = pickle.load(f)
        
        self.documents = self.metadata
        print(f"ğŸ“‚ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ ë¬¸ì„œ")


def build_rag_index():
    """RAG ì¸ë±ìŠ¤ êµ¬ì¶•"""
    engine = SemanticSearchEngineRAG()
    
    # docs í´ë”ì˜ ëª¨ë“  MD íŒŒì¼ ë¡œë”©
    all_docs = engine.load_all_markdown_files('docs')
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    engine.build_index(all_docs)
    
    # ì €ì¥
    engine.save_index()
    
    print(f"\nğŸ‰ RAG ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_docs)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±")
    
    return engine


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ alias
def build_semantic_search_index():
    """ì‹œë§¨í‹± ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (RAG ë²„ì „)"""
    return build_rag_index()


if __name__ == "__main__":
    print("="*60)
    print("  RAG ê¸°ë°˜ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶•")
    print("="*60)
    print()
    
    engine = build_rag_index()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n" + "="*60)
    print("  í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("="*60)
    
    test_queries = [
        "í•˜ê³„íœ´ê°€ëŠ” ì–¸ì œê¹Œì§€?",
        "VDI ì˜¤ë¥˜ í•´ê²° ë°©ë²•",
        "Mac PC ì´ë¦„ ë³€ê²½",
        "í”„ë¦°í„° IP ì£¼ì†Œ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ì§ˆë¬¸: {query}")
        results = engine.search(query, top_k=3)
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"  {i}. [{score:.3f}] {doc['title']}")
            print(f"      ë ˆë²¨: {doc['level']}, ë¶€ëª¨: {doc['parent']}")

