"""
ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ - 100% ë¬´ë£Œ, ë¡œì»¬ ì „ìš©
Sentence Transformers + FAISS ì‚¬ìš©
"""
import os
import json
import pickle
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class SemanticSearchEngine:
    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        """
        í•œêµ­ì–´ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        model: jhgan/ko-sroberta-multitask (í•œêµ­ì–´ ìµœì í™”)
        """
        print("ğŸ”„ ì‹œë§¨í‹± ê²€ìƒ‰ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.metadata = []
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def load_markdown_file(self, file_path: str) -> List[Dict]:
        """
        ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ë¡œ ë¶„í•  (H3 ë‹¨ìœ„ ì²­í‚¹)
        H2 = ì¹´í…Œê³ ë¦¬ (ë©”íƒ€ë°ì´í„°)
        H3 = ì‹¤ì œ ì²­í¬ ë‹¨ìœ„
        """
        print(f"ğŸ“„ MD íŒŒì¼ ë¡œë”©: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # H3 ë‹¨ìœ„ë¡œ ë¶„í• 
        chunks = []
        current_category = ""  # H2 ì¹´í…Œê³ ë¦¬
        current_section = ""   # H3 ì„¹ì…˜ ë‚´ìš©
        current_title = ""     # H3 ì œëª©
        
        for line in content.split('\n'):
            # [Page X] ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            if line.strip().startswith('[Page') and line.strip().endswith(']'):
                continue
            
            # H2: ì¹´í…Œê³ ë¦¬ ì €ì¥ (ì²­í¬ ë¶„ë¦¬ ì•ˆí•¨)
            if line.startswith('## ') and not line.startswith('###'):
                current_category = line.replace('##', '').strip()
                continue
            
            # H3: ì‹¤ì œ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            if line.startswith('### '):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section.strip() and current_title:
                    # [Page X] í…ìŠ¤íŠ¸ ì œê±°
                    cleaned_content = '\n'.join([
                        l for l in current_section.split('\n') 
                        if not (l.strip().startswith('[Page') and l.strip().endswith(']'))
                    ])
                    
                    chunks.append({
                        'category': current_category,  # H2 ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                        'title': current_title,
                        'content': cleaned_content.strip(),
                        'source': file_path
                    })
                
                # ìƒˆ H3 ì„¹ì…˜ ì‹œì‘
                current_title = line.replace('###', '').strip()
                current_section = ""
            else:
                # H3 ì„¹ì…˜ ë‚´ìš© ëˆ„ì 
                current_section += line + '\n'
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section.strip() and current_title:
            # [Page X] í…ìŠ¤íŠ¸ ì œê±°
            cleaned_content = '\n'.join([
                l for l in current_section.split('\n') 
                if not (l.strip().startswith('[Page') and l.strip().endswith(']'))
            ])
            
            chunks.append({
                'category': current_category,
                'title': current_title,
                'content': cleaned_content.strip(),
                'source': file_path
            })
        
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks
    
    def load_faq_data(self, faq_file: str = 'data/faq_data.json') -> List[Dict]:
        """
        FAQ ë°ì´í„°ë¥¼ ë¡œë“œ
        """
        print(f"ğŸ“‹ FAQ ë°ì´í„° ë¡œë”©: {faq_file}")
        
        with open(faq_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = []
        for faq in data.get('faqs', []):
            chunks.append({
                'title': faq['question'],
                'content': f"ì§ˆë¬¸: {faq['question']}\n\në‹µë³€: {faq['main_answer']}",
                'source': 'FAQ',
                'category': faq.get('category', ''),
                'department': faq.get('department', ''),
                'link': faq.get('link', ''),
                'faq_id': faq.get('id', 0)
            })
        
        print(f"âœ… {len(chunks)}ê°œ FAQ ë¡œë”© ì™„ë£Œ")
        return chunks
    
    def build_index(self, documents: List[Dict]):
        """
        ë¬¸ì„œë“¤ì„ ë²¡í„°í™”í•˜ì—¬ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        """
        print("ğŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        self.documents = documents
        self.metadata = documents
        
        # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì œëª© + ë‚´ìš©)
        texts = [f"{doc['title']}\n{doc['content']}" for doc in documents]
        
        # ë²¡í„°ë¡œ ë³€í™˜
        print("â³ ì„ë² ë”© ìƒì„± ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰)")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ë‚´ì (Inner Product) ì‚¬ìš©
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(embeddings)
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! (ì´ {len(documents)}ê°œ ë¬¸ì„œ)")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """
        ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        
        Returns:
            List of (document, similarity_score) tuples
        """
        if self.index is None:
            raise ValueError("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_index()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.model.encode([query])
        faiss.normalize_L2(query_vector)
        
        # ê²€ìƒ‰
        scores, indices = self.index.search(query_vector.astype('float32'), top_k)
        
        # ê²°ê³¼ ë°˜í™˜
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ ê²½ìš°
                results.append((self.metadata[idx], float(score)))
        
        return results
    
    def save_index(self, path: str = 'data/semantic_index'):
        """
        ì¸ë±ìŠ¤ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
        """
        os.makedirs(path, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, f'{path}/faiss.index')
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(f'{path}/metadata.pkl', 'wb') as f:
            pickle.dump(self.metadata, f)
        
        print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_index(self, path: str = 'data/semantic_index'):
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ
        """
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(f'{path}/faiss.index')
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f'{path}/metadata.pkl', 'rb') as f:
            self.metadata = pickle.load(f)
        
        print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ ë¬¸ì„œ")


def build_semantic_search_index():
    """
    ì‹œë§¨í‹± ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    """
    engine = SemanticSearchEngine()
    
    # ëª¨ë“  ë¬¸ì„œ ë¡œë“œ
    all_docs = []
    
    # 1. FAQ ë°ì´í„°
    all_docs.extend(engine.load_faq_data('data/faq_data.json'))
    
    # 2. ì—”ì¹´ìƒí™œê°€ì´ë“œ
    if os.path.exists('docs/ì—”ì¹´ìƒí™œê°€ì´ë“œ.md'):
        all_docs.extend(engine.load_markdown_file('docs/ì—”ì¹´ìƒí™œê°€ì´ë“œ.md'))
    
    # 3. ì¶”ê°€ MD íŒŒì¼ë“¤ (ìˆìœ¼ë©´)
    for md_file in ['docs/ITê°€ì´ë“œ.md', 'docs/ì‚¬ì—…ê°€ì´ë“œ.md']:
        if os.path.exists(md_file):
            all_docs.extend(engine.load_markdown_file(md_file))
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    engine.build_index(all_docs)
    
    # ì €ì¥
    engine.save_index()
    
    print(f"\nğŸ‰ ì‹œë§¨í‹± ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_docs)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±")
    
    return engine


if __name__ == "__main__":
    # ì¸ë±ìŠ¤ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸
    print("="*60)
    print("  ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶•")
    print("="*60)
    print()
    
    engine = build_semantic_search_index()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n" + "="*60)
    print("  í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("="*60)
    
    test_queries = [
        "íœ´ê°€ê°€ ì–¸ì œ ìƒê²¨?",
        "ì»´í“¨í„° ë¹„ë²ˆ ê¹Œë¨¹ì—ˆì–´",
        "íšŒì˜ì‹¤ ì˜ˆì•½ ì–´ë–»ê²Œ í•´?",
        "ì˜ì¹´ í• ì¸ ë°›ì„ ìˆ˜ ìˆì–´?"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ì§ˆë¬¸: {query}")
        results = engine.search(query, top_k=3)
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"  {i}. [{score:.3f}] {doc['title'][:50]}...")


