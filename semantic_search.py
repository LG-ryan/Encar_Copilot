"""
ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ - 100% ë¬´ë£Œ, ë¡œì»¬ ì „ìš©
Sentence Transformers + FAISS ì‚¬ìš©
"""
import os
import json
import pickle
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class SemanticSearchEngine:
    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        """
        í•œêµ­ì–´ ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        model: jhgan/ko-sroberta-multitask (í•œêµ­ì–´ ìµœì í™”)
        """
        print("ğŸ”„ ì‹œë§¨í‹± ê²€ìƒ‰ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.metadata = []
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def load_markdown_file(self, file_path: str) -> List[Dict]:
        """
        ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ë¡œ ë¶„í•  (ì§ˆë¬¸/ë‹µë³€ ë‹¨ìœ„ ì²­í‚¹)
        H2 = ì¹´í…Œê³ ë¦¬ (ë©”íƒ€ë°ì´í„°)
        H3 = ì„¹ì…˜ ì œëª©
        **ì§ˆë¬¸:** / **ë‹µë³€:** = ì‹¤ì œ ê²€ìƒ‰ ë‹¨ìœ„
        """
        print(f"ğŸ“„ MD íŒŒì¼ ë¡œë”©: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # H3 ë‹¨ìœ„ë¡œ ë¶„í• 
        chunks = []
        current_category = ""  # H2 ì¹´í…Œê³ ë¦¬
        current_section = ""   # H3 ì„¹ì…˜ ë‚´ìš©
        current_title = ""     # H3 ì œëª©
        
        for line in content.split('\n'):
            # [Page X] ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            if line.strip().startswith('[Page') and line.strip().endswith(']'):
                continue
            
            # ì´ë¯¸ì§€ ë¼ì¸ ê±´ë„ˆë›°ê¸° (![ì´ë¯¸ì§€](data:image ë˜ëŠ” ![ì´ë¯¸ì§€](page)
            if line.strip().startswith('![ì´ë¯¸ì§€](data:image') or line.strip().startswith('![ì´ë¯¸ì§€](page'):
                continue
            
            # H2: ì¹´í…Œê³ ë¦¬ ì €ì¥ (ì²­í¬ ë¶„ë¦¬ ì•ˆí•¨)
            if line.startswith('## ') and not line.startswith('###'):
                current_category = line.replace('##', '').strip()
                continue
            
            # H3: ì‹¤ì œ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            if line.startswith('### '):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section.strip() and current_title:
                    chunks.extend(
                        self._parse_qa_section(current_title, current_section, current_category, file_path)
                    )
                
                # ìƒˆ H3 ì„¹ì…˜ ì‹œì‘
                current_title = line.replace('###', '').strip()
                current_section = ""
            else:
                # H3 ì„¹ì…˜ ë‚´ìš© ëˆ„ì 
                current_section += line + '\n'
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section.strip() and current_title:
            chunks.extend(
                self._parse_qa_section(current_title, current_section, current_category, file_path)
            )
        
        print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks
    
    def _parse_qa_section(self, title: str, content: str, category: str, source: str) -> List[Dict]:
        """
        H3 ì„¹ì…˜ì„ ì§ˆë¬¸/ë‹µë³€ ë‹¨ìœ„ë¡œ íŒŒì‹± (ë‹¤ì¤‘ ì§ˆë¬¸ ì§€ì›)
        
        ì˜ˆì‹œ:
        ### PC-OFF í”„ë¡œê·¸ë¨
        **ì§ˆë¬¸:** PC-OFF í”„ë¡œê·¸ë¨ì´ ë­ì•¼?
        **ë‹µë³€:** ...
        **ì§ˆë¬¸:** ì‚¬ìš© ë°©ë²•ì€?
        **ë‹µë³€:** ...
        
        â†’ ê° ì§ˆë¬¸/ë‹µë³€ ìŒì„ ê°œë³„ ì²­í¬ë¡œ ìƒì„±
        """
        chunks = []
        
        # [Page X] í…ìŠ¤íŠ¸ ì œê±°
        cleaned_content = '\n'.join([
            l for l in content.split('\n') 
            if not (l.strip().startswith('[Page') and l.strip().endswith(']'))
        ]).strip()
        
        # **ì§ˆë¬¸:**ê³¼ **ë‹µë³€:** íŒ¨í„´ í™•ì¸
        if '**ì§ˆë¬¸:**' in cleaned_content and '**ë‹µë³€:**' in cleaned_content:
            # ì •ê·œì‹ìœ¼ë¡œ ì§ˆë¬¸/ë‹µë³€ ìŒ ì¶”ì¶œ
            import re
            
            # **ì§ˆë¬¸:** ìœ¼ë¡œ ë¶„í• 
            parts = re.split(r'\*\*ì§ˆë¬¸:\*\*', cleaned_content)
            
            for part in parts[1:]:  # ì²« ë¶€ë¶„ì€ ì§ˆë¬¸ ì „ ë‚´ìš©ì´ë¯€ë¡œ ìŠ¤í‚µ
                if '**ë‹µë³€:**' not in part:
                    continue
                    
                # ì§ˆë¬¸ê³¼ ë‹µë³€ ë¶„ë¦¬
                qa_split = part.split('**ë‹µë³€:**', 1)
                if len(qa_split) != 2:
                    continue
                    
                question = qa_split[0].strip()
                answer_raw = qa_split[1]
                
                # ë‹¤ìŒ **ì§ˆë¬¸:** ì „ê¹Œì§€ê°€ ë‹µë³€
                next_question_match = re.search(r'\*\*ì§ˆë¬¸:\*\*', answer_raw)
                if next_question_match:
                    answer = answer_raw[:next_question_match.start()].strip()
                else:
                    answer = answer_raw.strip()
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ì§ˆë¬¸ + ì¹´í…Œê³ ë¦¬ + ì„¹ì…˜)
                keywords = self._extract_keywords(f"{category} {title} {question}")
                
                # ì²­í¬ ìƒì„±: ê° ì§ˆë¬¸/ë‹µë³€ì„ ë…ë¦½ëœ ì²­í¬ë¡œ
                chunks.append({
                    'h1': 'ì—”ì¹´ìƒí™œê°€ì´ë“œ',  # ë¬¸ì„œëª…
                    'h2': category,  # ëŒ€ë¶„ë¥˜ (ì˜ˆ: "ê·¼íƒœ ë° íœ´ê°€")
                    'h3': title,  # ì¤‘ë¶„ë¥˜ (ì˜ˆ: "PC-OFF í”„ë¡œê·¸ë¨")
                    'section': title,  # í•˜ìœ„ í˜¸í™˜
                    'title': question,  # ì†Œë¶„ë¥˜ (ì§ˆë¬¸ ìì²´)
                    'question': question,
                    'answer': answer,
                    'content': f"**ì§ˆë¬¸:** {question}\n\n**ë‹µë³€:** {answer}",
                    'keywords': keywords,
                    'parent_section': category,  # ë¶€ëª¨ ì¹´í…Œê³ ë¦¬
                    'source': source,
                    'chunk_type': 'qa'  # ì²­í¬ íƒ€ì… ëª…ì‹œ
                })
        else:
            # ì§ˆë¬¸/ë‹µë³€ íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ ì„¤ëª… ì„¹ì…˜)
            keywords = self._extract_keywords(f"{category} {title} {cleaned_content[:100]}")
            
            chunks.append({
                'h1': 'ì—”ì¹´ìƒí™œê°€ì´ë“œ',
                'h2': category,
                'h3': title,
                'section': title,
                'title': title,
                'content': cleaned_content,
                'keywords': keywords,
                'parent_section': category,
                'source': source,
                'chunk_type': 'section'  # ì¼ë°˜ ì„¹ì…˜
            })
        
        return chunks
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í°í™”)
        """
        import re
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        # 2ê¸€ì ì´ìƒë§Œ í•„í„°ë§ + ì¤‘ë³µ ì œê±°
        keywords = list(set([w for w in words if len(w) >= 2]))
        return keywords[:20]  # ìµœëŒ€ 20ê°œ
    
    def load_faq_data(self, faq_file: str = 'data/faq_data.json') -> List[Dict]:
        """
        FAQ ë°ì´í„°ë¥¼ ë¡œë“œ
        """
        print(f"ğŸ“‹ FAQ ë°ì´í„° ë¡œë”©: {faq_file}")
        
        with open(faq_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = []
        for faq in data.get('faqs', []):
            chunks.append({
                'title': faq['question'],
                'content': f"ì§ˆë¬¸: {faq['question']}\n\në‹µë³€: {faq['main_answer']}",
                'source': 'FAQ',
                'category': faq.get('category', ''),
                'department': faq.get('department', ''),
                'link': faq.get('link', ''),
                'faq_id': faq.get('id', 0)
            })
        
        print(f"âœ… {len(chunks)}ê°œ FAQ ë¡œë”© ì™„ë£Œ")
        return chunks
    
    def build_index(self, documents: List[Dict]):
        """
        ë¬¸ì„œë“¤ì„ ë²¡í„°í™”í•˜ì—¬ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        """
        print("ğŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        self.documents = documents
        self.metadata = documents
        
        # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì œëª© + ë‚´ìš©)
        texts = [f"{doc['title']}\n{doc['content']}" for doc in documents]
        
        # ë²¡í„°ë¡œ ë³€í™˜
        print("â³ ì„ë² ë”© ìƒì„± ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ìˆ˜í–‰)")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ë‚´ì (Inner Product) ì‚¬ìš©
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(embeddings)
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embeddings.astype('float32'))
        
        print(f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! (ì´ {len(documents)}ê°œ ë¬¸ì„œ)")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """
        ìì—°ì–´ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        
        Returns:
            List of (document, similarity_score) tuples
        """
        if self.index is None:
            raise ValueError("ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_index()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.model.encode([query])
        faiss.normalize_L2(query_vector)
        
        # ê²€ìƒ‰
        scores, indices = self.index.search(query_vector.astype('float32'), top_k)
        
        # ê²°ê³¼ ë°˜í™˜
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ ê²½ìš°
                results.append((self.metadata[idx], float(score)))
        
        return results
    
    def save_index(self, path: str = 'data/semantic_index'):
        """
        ì¸ë±ìŠ¤ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
        """
        os.makedirs(path, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, f'{path}/faiss.index')
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(f'{path}/metadata.pkl', 'wb') as f:
            pickle.dump(self.metadata, f)
        
        print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_index(self, path: str = 'data/semantic_index'):
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ
        """
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(f'{path}/faiss.index')
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f'{path}/metadata.pkl', 'rb') as f:
            self.metadata = pickle.load(f)
        
        print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.metadata)}ê°œ ë¬¸ì„œ")


def build_semantic_search_index():
    """
    ì‹œë§¨í‹± ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    """
    engine = SemanticSearchEngine()
    
    # ëª¨ë“  ë¬¸ì„œ ë¡œë“œ
    all_docs = []
    
    # 1. FAQ ë°ì´í„°
    all_docs.extend(engine.load_faq_data('data/faq_data.json'))
    
    # 2. ì—”ì¹´ìƒí™œê°€ì´ë“œ
    if os.path.exists('docs/ì—”ì¹´ìƒí™œê°€ì´ë“œ.md'):
        all_docs.extend(engine.load_markdown_file('docs/ì—”ì¹´ìƒí™œê°€ì´ë“œ.md'))
    
    # 3. ì¶”ê°€ MD íŒŒì¼ë“¤ (ìˆìœ¼ë©´)
    for md_file in ['docs/ITê°€ì´ë“œ.md', 'docs/ì‚¬ì—…ê°€ì´ë“œ.md']:
        if os.path.exists(md_file):
            all_docs.extend(engine.load_markdown_file(md_file))
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    engine.build_index(all_docs)
    
    # ì €ì¥
    engine.save_index()
    
    print(f"\nğŸ‰ ì‹œë§¨í‹± ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(all_docs)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±")
    
    return engine


if __name__ == "__main__":
    # ì¸ë±ìŠ¤ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸
    print("="*60)
    print("  ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ êµ¬ì¶•")
    print("="*60)
    print()
    
    engine = build_semantic_search_index()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n" + "="*60)
    print("  í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("="*60)
    
    test_queries = [
        "íœ´ê°€ê°€ ì–¸ì œ ìƒê²¨?",
        "ì»´í“¨í„° ë¹„ë²ˆ ê¹Œë¨¹ì—ˆì–´",
        "íšŒì˜ì‹¤ ì˜ˆì•½ ì–´ë–»ê²Œ í•´?",
        "ì˜ì¹´ í• ì¸ ë°›ì„ ìˆ˜ ìˆì–´?"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ì§ˆë¬¸: {query}")
        results = engine.search(query, top_k=3)
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"  {i}. [{score:.3f}] {doc['title'][:50]}...")


